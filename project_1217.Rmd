---
title: "project_1217"
output: html_document
date: "2025-12-17"
---

```{r}
data = read.csv('project_data_1217.csv')
head(data)
```

```{r}
df = data
df$has_garage_label <- factor(df$has_garage, 
                               levels = c(0, 1),
                               labels = c("No Garage", "Has Garage"))

df$has_dropoff_label <- factor(df$has_dropoff, 
                               levels = c(0, 1),
                              labels = c("No Dropoff", "Has Dropoff"))

```

```{r}

#stripchart(inspections ~ has_garage_label, vertical=TRUE, method='jitter', pch=16,
#           data = df,
#           main = "Rat Inspections by Garage Presence",
#           xlab = "Garage Status",
#           ylab = "Number of Inspections")

#stripchart(inspections ~ has_dropoff_label, vertical=TRUE, method='jitter', pch=16,
#           data = df,
#           main = "Rat Inspections by Dropoff Presence",
#           xlab = "Dropoff Status",
#           ylab = "Number of Inspections")
```

```{r}
# Simple Linear Regression
model <- lm(inspections ~ population, data=df)
summary(model)

plot(df$population, df$inspections, xlab='Population', ylab='Rat Inspections', 
     main='Population vs. Rat Inspections by Zip Code')
abline(model, col='red')

# Full regression model
full_model <- lm(inspections ~ population + has_garage + has_dropoff, data=df)
summary(full_model)
```

```{r}

corr_data <- regression_with_parks_df %>% 
  select(inspections, population, has_garage, has_dropoff, total_park_acres)
corr_matrix <- cor(corr_data, use = "complete.obs")
kable(round(corr_matrix, 2), caption = "Correlation Matrix for Key Variables")

corrplot(
  corr_matrix,
  method = "color",
  type = "upper",              # Only show upper half
  addCoef.col = "black",       # Add correlation coefficients
  tl.col = "black",            # Text color
  tl.srt = 45,                 # Label rotation
  number.cex = 0.7,            # Size of correlation numbers
  col = colorRampPalette(c("skyblue", "white", "tomato"))(200),
  title = "Correlation Matrix of Key Variables",
  mar = c(0,0,2,0)
)

```


```{r}
#interaction plots 
library(ggplot2)

ggplot(df, aes(x = population, y = inspections, color = factor(has_dropoff))) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    color = "Has Drop-off",
    title = "Rat Inspections vs Population by Food Scrap Drop-off Presence"
  )


ggplot(df, aes(x = population, y = inspections, color = factor(has_garage))) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    color = "Has Garage",
    title = "Rat Inspections vs Population by DSNY Garage Presence"
  )

#litter_bin is continuous so binning it in order to assess interaction
df$litter_bin = cut(
  df$litter_basket_count,
  breaks = quantile(df$litter_basket_count, probs = c(0, .33, .66, 1)),
  include.lowest = TRUE
)

ggplot(df, aes(x = population, y = inspections, color = litter_bin)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE)

```

```{r}

#TESTING
df_modeling = df[, -c(1,8,9)]

full_model = lm(inspections ~ (.)^2, data = df_modeling ); #summary(full_model)
red_1 = lm(inspections ~., data = df_modeling); #summary(red_1)
red_2 = lm(inspections ~. + I(population * has_dropoff) + I(population * has_garage) + I(population * litter_basket_count), data = df_modeling); #summary(red_2)

#dropping park acres 
#df_modeling2 = df_modeling[, -6]
#red_3 = lm(inspections ~ (.)^2 , data = df_modeling2); summary(red_3)

```

```{r}

box_cox_base_model = lm(inspections ~., data = df_modeling); summary(box_cox_base_model)
shapiro.test(rstudent(box_cox_base_model))
bptest(box_cox_base_model)

bc = boxcox(box_cox_base_model, lambda = seq(-1, 1, by = 0.05), plotit = TRUE)
lambda_hat = bc$x[which.max(bc$y)]; lambda_hat

df_modeling2 = df_modeling
df_modeling2$log_inspections = log(df_modeling$inspections)
df_modeling2 = df_modeling2[, -1]

log_base_model = lm(log_inspections ~ ., data = df_modeling2); summary(log_base_model)

log_full_model = lm(log_inspections ~ (.)^2, data = df_modeling2); summary(log_full_model)

log_model = lm(log_inspections ~ population * has_dropoff + population * has_garage + population * litter_basket_count, data = df_modeling2); summary(log_model)


### STUDENTIZED DELETED RESIDUALS LOG BASE MODEL
stu_del_resid = rstudent(log_base_model)
##QQ Plot
qqnorm(stu_del_resid, 
       pch = 20, col = "navy", cex = 1.0,
       main = "QQPlot: Studentized Deleted Residuals")
qqline(stu_del_resid, 
       lwd = 1, col = "maroon")
##Histogram 
hist(stu_del_resid,
     col = "cadetblue2", border = "darkblue", lwd = 1,
     main = "Histogram: Studentized Deleted Residuals",
     xlab = "Studentized Deleted Residuals")
##Line Plot
plot(stu_del_resid,
     type = 'l', col = "cadetblue", lwd = 2,
     main = "Line Plot: Studentized Deleted Residuals",
     ylab = "Studentized Deleted Residuals")
abline(h = 0, lwd = 1, lty = 2, col = "darkblue")
##vs predicted values 
plot(x = fitted(model), y = stu_del_resid,
     pch = 16, col = "deepskyblue4", cex = 1.2,
     main = "Studentized Deleted Residuals vs. Predicted (Fitted) Values",
     ylab = "Studentized Deleted Residuals",
     xlab = "Predicted (Fitted) Values")
abline(h = 0, lwd = 1, lty = 2, col = "maroon")

shapiro.test(rstudent(log_base_model))
max(cooks.distance(log_base_model))
library(lmtest)
bptest(log_base_model)

PRESS = sum((residuals(log_model) / (1 - hatvalues(log_model)))^2)
MSPR = PRESS / length(residuals(log_model))
MSPR
MSE = mean(residuals(log_model)^2)
MSE

install.packages('car')
library(car)
vif(log_model, type = 'predictor')



### STUDENTIZED DELETED RESIDUALS FINAL MODEL 
stu_del_resid = rstudent(log_model)
##QQ Plot
qqnorm(stu_del_resid, 
       pch = 20, col = "navy", cex = 1.0,
       main = "QQPlot: Studentized Deleted Residuals")
qqline(stu_del_resid, 
       lwd = 1, col = "maroon")
##Histogram 
hist(stu_del_resid,
     col = "cadetblue2", border = "darkblue", lwd = 1,
     main = "Histogram: Studentized Deleted Residuals",
     xlab = "Studentized Deleted Residuals")
##Line Plot
plot(stu_del_resid,
     type = 'l', col = "cadetblue", lwd = 2,
     main = "Line Plot: Studentized Deleted Residuals",
     ylab = "Studentized Deleted Residuals")
abline(h = 0, lwd = 1, lty = 2, col = "darkblue")
##vs predicted values 
plot(x = fitted(model), y = stu_del_resid,
     pch = 16, col = "deepskyblue4", cex = 1.2,
     main = "Studentized Deleted Residuals vs. Predicted (Fitted) Values",
     ylab = "Studentized Deleted Residuals",
     xlab = "Predicted (Fitted) Values")
abline(h = 0, lwd = 1, lty = 2, col = "maroon")

shapiro.test(rstudent(log_model))
max(cooks.distance(log_model))
library(lmtest)
bptest(log_model)

PRESS = sum((residuals(log_model) / (1 - hatvalues(log_model)))^2)
MSPR = PRESS / length(residuals(log_model))
MSPR
MSE = mean(residuals(log_model)^2)
MSE

install.packages('car')
library(car)
vif(log_model, type = 'predictor')
```
